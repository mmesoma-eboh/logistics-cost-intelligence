<jupyter_text>
Logistics Cost Analysis - Feature Engineering & Advanced AnalyticsThis notebook creates advanced features for forecasting and builds the final analytical dataset.
<jupyter_code>
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression
import warnings
warnings.filterwarnings('ignore')

print("âœ… Libraries imported successfully!")
<jupyter_output>
âœ… Libraries imported successfully!
<jupyter_text>
Load and Prepare Data
<jupyter_code>
# Load cleaned data
standard_freight = pd.read_csv('../data/processed/standard_freight_data.csv')
standard_freight['ship_date'] = pd.to_datetime(standard_freight['ship_date'])

print("ğŸ“Š Data loaded successfully!")
print(f"Dataset shape: {standard_freight.shape}")
standard_freight[['ship_date', 'lane', 'invoice_amount', 'cleaned_billable_weight', 'cleaned_cost_per_kg']].head()
<jupyter_output>
ğŸ“Š Data loaded successfully!
Dataset shape: (93, 11)
<jupyter_text>
1. Create Time-Based Features
<jupyter_code>
# Extract time components
standard_freight['month'] = standard_freight['ship_date'].dt.month
standard_freight['quarter'] = standard_freight['ship_date'].dt.quarter
standard_freight['day_of_week'] = standard_freight['ship_date'].dt.dayofweek
standard_freight['is_weekend'] = standard_freight['day_of_week'].isin([5, 6]).astype(int)

print("ğŸ“… Time-based features created:")
print(standard_freight[['ship_date', 'month', 'quarter', 'day_of_week', 'is_weekend']].head())
<jupyter_output>
ğŸ“… Time-based features created:
   ship_date  month  quarter  day_of_week  is_weekend
0 2024-01-01      1        1            0           0
1 2024-01-02      1        1            1           0
2 2024-01-03      1        1            2           0
3 2024-01-04      1        1            3           0
4 2024-01-05      1        1            4           0
<jupyter_text>
2. Create Lane-Specific Features
<jupyter_code>
# Lane efficiency metrics
lane_metrics = standard_freight.groupby('lane').agg({
    'cleaned_cost_per_kg': ['mean', 'std'],
    'cleaned_billable_weight': 'mean',
    'shipment_id': 'count'
}).round(2)

lane_metrics.columns = ['lane_avg_cost', 'lane_cost_std', 'lane_avg_weight', 'lane_shipment_count']
standard_freight = standard_freight.merge(lane_metrics, left_on='lane', right_index=True)

# Create lane efficiency score (lower cost per kg = more efficient)
standard_freight['lane_efficiency_score'] = (
    standard_freight['lane_avg_cost'] / standard_freight['cleaned_cost_per_kg']
)

print("ğŸ›£ï¸ Lane-specific features created:")
print(standard_freight[['lane', 'lane_avg_cost', 'lane_cost_std', 'lane_efficiency_score']].head())
<jupyter_output>
ğŸ›£ï¸ Lane-specific features created:
            lane  lane_avg_cost  lane_cost_std  lane_efficiency_score
0       NYC-FRA          40.13          21.66               0.917244
1  SHANGHAI-CHICAGO          21.80           9.34               0.753425
2  LONDON-SINGAPORE          23.71           9.94               1.000000
3       NYC-FRA          40.13          21.66               1.003740
4  SHANGHAI-CHICAGO          21.80           9.34               0.763441
<jupyter_text>
3. Create Consolidation Opportunity Features
<jupyter_code>
# Identify poor consolidation patterns
standard_freight['is_small_shipment'] = (
    standard_freight['cleaned_billable_weight'] < 20
).astype(int)

standard_freight['is_high_cost_small'] = (
    (standard_freight['is_small_shipment'] == 1) & 
    (standard_freight['cleaned_cost_per_kg'] > standard_freight['lane_avg_cost'])
).astype(int)

# Calculate consolidation score (higher = more consolidation needed)
standard_freight['consolidation_opportunity_score'] = (
    standard_freight['is_high_cost_small'] * 
    (standard_freight['cleaned_cost_per_kg'] / standard_freight['lane_avg_cost'])
)

print("ğŸ“¦ Consolidation features created:")
consolidation_summary = standard_freight.groupby('lane').agg({
    'is_small_shipment': 'sum',
    'is_high_cost_small': 'sum',
    'consolidation_opportunity_score': 'sum'
})
print(consolidation_summary)
<jupyter_output>
ğŸ“¦ Consolidation features created:
                  is_small_shipment  is_high_cost_small  \
lane                                                     
LONDON-SINGAPORE                 15                   5   
NYC-FRA                          24                  15   
SHANGHAI-CHICAGO                 17                   7   

                  consolidation_opportunity_score  
lane                                               
LONDON-SINGAPORE                             6.36  
NYC-FRA                                     25.10  
SHANGHAI-CHICAGO                             8.61
<jupyter_text>
4. Service Level Premium Analysis
<jupyter_code>
# Calculate service level premiums
service_premiums = standard_freight.groupby('service_level').agg({
    'cleaned_cost_per_kg': 'mean',
    'invoice_amount': 'mean'
}).round(2)

base_cost = service_premiums.loc['Standard', 'cleaned_cost_per_kg']
service_premiums['premium_multiplier'] = (service_premiums['cleaned_cost_per_kg'] / base_cost).round(2)

print("ğŸ’° Service Level Premium Analysis:")
print(service_premiums)

# Add premium feature to main dataset
premium_map = service_premiums['premium_multiplier'].to_dict()
standard_freight['service_level_premium'] = standard_freight['service_level'].map(premium_map)
<jupyter_output>
ğŸ’° Service Level Premium Analysis:
           cleaned_cost_per_kg  invoice_amount  premium_multiplier
service_level                                                      
Express                  42.86         2257.14                1.85
Priority                 52.50         2550.00                2.26
Standard                 23.24         1602.70                1.00
<jupyter_text>
5. Feature Selection for Forecasting
<jupyter_code>
# Prepare features for correlation analysis
features_for_selection = standard_freight[[
    'cleaned_billable_weight', 'month', 'quarter', 'day_of_week', 
    'is_weekend', 'lane_avg_cost', 'lane_cost_std', 'lane_avg_weight',
    'is_small_shipment', 'service_level_premium'
]].copy()

# Handle categorical variables (lane)
lane_dummies = pd.get_dummies(standard_freight['lane'], prefix='lane')
features_for_selection = pd.concat([features_for_selection, lane_dummies], axis=1)

target = standard_freight['cleaned_cost_per_kg']

print("ğŸ” Feature correlation with cost per kg:")
correlations = features_for_selection.corrwith(target).sort_values(ascending=False)
for feature, corr in correlations.items():
    print(f"{feature:25} : {corr:+.3f}")
<jupyter_output>
ğŸ” Feature correlation with cost per kg:
service_level_premium     : +0.527
lane_avg_cost             : +0.369
lane_NYC-FRA              : +0.324
is_small_shipment         : +0.230
lane_cost_std             : +0.224
lane_LONDON-SINGAPORE     : -0.014
quarter                   : -0.019
month                     : -0.019
lane_avg_weight           : -0.073
is_weekend                : -0.080
day_of_week               : -0.083
cleaned_billable_weight   : -0.107
lane_SHANGHAI-CHICAGO     : -0.310
<jupyter_text>
6. Create Final Analytical Dataset
<jupyter_code>
# Select top features for modeling
selected_features = features_for_selection[[
    'service_level_premium', 'lane_avg_cost', 'is_small_shipment', 
    'cleaned_billable_weight', 'lane_NYC-FRA'
]]

# Add target variable
final_dataset = pd.concat([selected_features, target], axis=1)

print("ğŸ¯ Final analytical dataset created!")
print(f"Shape: {final_dataset.shape}")
print("\nSelected features:")
print(list(final_dataset.columns[:-1]))
final_dataset.head()
<jupyter_output>
ğŸ¯ Final analytical dataset created!
Shape: (93, 6)

Selected features:
['service_level_premium', 'lane_avg_cost', 'is_small_shipment', 'cleaned_billable_weight', 'lane_NYC-FRA']
<jupyter_text>
7. Save Enhanced Dataset
<jupyter_code>
# Save the final dataset for modeling
final_dataset.to_csv('../data/processed/final_analytical_dataset.csv', index=False)

# Also save the full enhanced dataset
enhanced_dataset = standard_freight.copy()
enhanced_dataset.to_csv('../data/processed/enhanced_logistics_data.csv', index=False)

print("ğŸ’¾ Datasets saved successfully!")
print("ğŸ“ Files created:")
print("- ../data/processed/final_analytical_dataset.csv")
print("- ../data/processed/enhanced_logistics_data.csv")
<jupyter_output>
ğŸ’¾ Datasets saved successfully!
ğŸ“ Files created:
- ../data/processed/final_analytical_dataset.csv
- ../data/processed/enhanced_logistics_data.csv
<jupyter_text>
8. Feature Importance Summary
<jupyter_code>
print("ğŸ¯ FEATURE ENGINEERING SUMMARY")
print("=" * 50)
print("Top 5 Most Important Features for Cost Prediction:")
print("1. Service Level Premium (correlation: +0.527)")
print("2. Lane Average Cost (correlation: +0.369)") 
print("3. NYC-FRA Lane Indicator (correlation: +0.324)")
print("4. Small Shipment Flag (correlation: +0.230)")
print("5. Billable Weight (correlation: -0.107)")

print(f"\nğŸ“Š Consolidation Opportunities Identified:")
print(f"- NYC-FRA lane: {consolidation_summary.loc['NYC-FRA', 'is_high_cost_small']} high-cost small shipments")
print(f"- Total consolidation score: {consolidation_summary['consolidation_opportunity_score'].sum():.1f}")

print(f"\nğŸ’° Service Level Premiums:")
for service, multiplier in premium_map.items():
    print(f"- {service}: {multiplier}x standard cost")